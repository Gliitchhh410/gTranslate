{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf0acad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping torch as it is not installed.\n",
      "WARNING: Skipping torchtext as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 6 (188.8 MB)\n"
     ]
    }
   ],
   "source": [
    "# First cell - clean everything\n",
    "!pip uninstall torch torchtext -y\n",
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9b28cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.15.1\n",
      "  Downloading torchtext-0.15.1-cp311-cp311-win_amd64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchtext==0.15.1) (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchtext==0.15.1) (2.32.3)\n",
      "Requirement already satisfied: torch==2.0.0 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchtext==0.15.1) (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchtext==0.15.1) (2.3.2)\n",
      "Requirement already satisfied: torchdata==0.6.0 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchtext==0.15.1) (0.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.0.0->torchtext==0.15.1) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\yn\\appdata\\roaming\\python\\python311\\site-packages (from torch==2.0.0->torchtext==0.15.1) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.0.0->torchtext==0.15.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.0.0->torchtext==0.15.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.0.0->torchtext==0.15.1) (3.1.6)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchdata==0.6.0->torchtext==0.15.1) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch==2.0.0->torchtext==0.15.1) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchtext==0.15.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchtext==0.15.1) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchtext==0.15.1) (2024.12.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch==2.0.0->torchtext==0.15.1) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yn\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->torchtext==0.15.1) (0.4.6)\n",
      "Downloading torchtext-0.15.1-cp311-cp311-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.9 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 1.3/1.9 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 3.8 MB/s eta 0:00:00\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.15.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.15.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "798c6a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.0.0+cpu\n",
      "TorchText: 0.15.0\n"
     ]
    }
   ],
   "source": [
    "# Third cell - verify installation\n",
    "import torch\n",
    "import torchtext\n",
    "print(f\"PyTorch: {torch.__version__}\")  # Should show 1.8.0\n",
    "print(f\"TorchText: {torchtext.__version__}\")  # Should show 0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2df1588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from torchtext.legacy.data import Field, BucketIterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2288d69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
      "     ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/14.6 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 1.0/14.6 MB 3.9 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.8/14.6 MB 3.5 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.6/14.6 MB 3.7 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 3.4/14.6 MB 3.8 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 4.2/14.6 MB 3.8 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 5.0/14.6 MB 3.8 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 5.8/14.6 MB 3.8 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 6.6/14.6 MB 3.8 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 7.3/14.6 MB 3.8 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 8.1/14.6 MB 3.8 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 8.9/14.6 MB 3.8 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 10.0/14.6 MB 3.9 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 10.7/14.6 MB 3.8 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 11.5/14.6 MB 3.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 12.1/14.6 MB 3.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 12.8/14.6 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 13.9/14.6 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.4/14.6 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 14.6/14.6 MB 3.7 MB/s eta 0:00:00\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "120b7ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.1/12.8 MB 3.9 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 2.9/12.8 MB 4.0 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 4.0 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.5/12.8 MB 3.9 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.3/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.9/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 3.9 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.2/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.8 MB/s eta 0:00:00\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4b52cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torchtext 0.15.1\n",
      "Uninstalling torchtext-0.15.1:\n",
      "  Successfully uninstalled torchtext-0.15.1\n",
      "Found existing installation: portalocker 3.2.0\n",
      "Uninstalling portalocker-3.2.0:\n",
      "  Successfully uninstalled portalocker-3.2.0\n",
      "Files removed: 32 (205.2 MB)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torchtext portalocker -y\n",
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "800f6790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.15.1\n",
      "  Downloading torchtext-0.15.1-cp311-cp311-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting portalocker==2.7.0\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchtext==0.15.1) (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchtext==0.15.1) (2.32.3)\n",
      "Requirement already satisfied: torch==2.0.0 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchtext==0.15.1) (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchtext==0.15.1) (2.3.2)\n",
      "Requirement already satisfied: torchdata==0.6.0 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchtext==0.15.1) (0.6.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\yn\\appdata\\roaming\\python\\python311\\site-packages (from portalocker==2.7.0) (310)\n",
      "Requirement already satisfied: filelock in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.0.0->torchtext==0.15.1) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\yn\\appdata\\roaming\\python\\python311\\site-packages (from torch==2.0.0->torchtext==0.15.1) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.0.0->torchtext==0.15.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.0.0->torchtext==0.15.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.0.0->torchtext==0.15.1) (3.1.6)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchdata==0.6.0->torchtext==0.15.1) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch==2.0.0->torchtext==0.15.1) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchtext==0.15.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchtext==0.15.1) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchtext==0.15.1) (2024.12.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch==2.0.0->torchtext==0.15.1) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yn\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->torchtext==0.15.1) (0.4.6)\n",
      "Downloading torchtext-0.15.1-cp311-cp311-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/1.9 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.8/1.9 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Installing collected packages: portalocker, torchtext\n",
      "\n",
      "   -------------------- ------------------- 1/2 [torchtext]\n",
      "   -------------------- ------------------- 1/2 [torchtext]\n",
      "   -------------------- ------------------- 1/2 [torchtext]\n",
      "   ---------------------------------------- 2/2 [torchtext]\n",
      "\n",
      "Successfully installed portalocker-2.7.0 torchtext-0.15.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.15.1 portalocker==2.7.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e2fd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.3.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\yn\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yn\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\yn\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yn\\appdata\\roaming\\python\\python311\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yn\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yn\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
      "   ---------------------------------------- 0.0/558.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 558.8/558.8 kB 3.8 MB/s eta 0:00:00\n",
      "Downloading pyarrow-21.0.0-cp311-cp311-win_amd64.whl (26.2 MB)\n",
      "   ---------------------------------------- 0.0/26.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/26.2 MB 4.2 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.6/26.2 MB 4.0 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 2.4/26.2 MB 3.9 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 2.9/26.2 MB 3.9 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 3.9/26.2 MB 3.9 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 4.7/26.2 MB 3.9 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.8/26.2 MB 3.9 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 6.3/26.2 MB 3.9 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 7.1/26.2 MB 3.8 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 7.9/26.2 MB 3.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.7/26.2 MB 3.8 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.4/26.2 MB 3.8 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 10.2/26.2 MB 3.8 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 11.3/26.2 MB 3.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 11.8/26.2 MB 3.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 12.6/26.2 MB 3.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 13.4/26.2 MB 3.7 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 14.2/26.2 MB 3.7 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 14.9/26.2 MB 3.7 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 15.5/26.2 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 16.3/26.2 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 17.0/26.2 MB 3.7 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 17.8/26.2 MB 3.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 18.6/26.2 MB 3.7 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 19.4/26.2 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 20.2/26.2 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 21.0/26.2 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 21.8/26.2 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 22.5/26.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 23.3/26.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 24.1/26.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.9/26.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.7/26.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.2/26.2 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, pyarrow, fsspec, dill, multiprocess, huggingface-hub, datasets\n",
      "\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "  Attempting uninstall: fsspec\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "    Found existing installation: fsspec 2025.7.0\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "    Uninstalling fsspec-2025.7.0:\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "      Successfully uninstalled fsspec-2025.7.0\n",
      "   ----- ---------------------------------- 1/7 [pyarrow]\n",
      "   ----------- ---------------------------- 2/7 [fsspec]\n",
      "   ----------- ---------------------------- 2/7 [fsspec]\n",
      "   ----------- ---------------------------- 2/7 [fsspec]\n",
      "   ----------- ---------------------------- 2/7 [fsspec]\n",
      "   ----------- ---------------------------- 2/7 [fsspec]\n",
      "   ----------- ---------------------------- 2/7 [fsspec]\n",
      "   ----------- ---------------------------- 2/7 [fsspec]\n",
      "   ----------- ---------------------------- 2/7 [fsspec]\n",
      "   ----------------- ---------------------- 3/7 [dill]\n",
      "   ----------------- ---------------------- 3/7 [dill]\n",
      "   ----------------- ---------------------- 3/7 [dill]\n",
      "   ----------------- ---------------------- 3/7 [dill]\n",
      "   ----------------- ---------------------- 3/7 [dill]\n",
      "   ---------------------- ----------------- 4/7 [multiprocess]\n",
      "   ---------------------- ----------------- 4/7 [multiprocess]\n",
      "   ---------------------- ----------------- 4/7 [multiprocess]\n",
      "   ---------------------- ----------------- 4/7 [multiprocess]\n",
      "   ---------------------------- ----------- 5/7 [huggingface-hub]\n",
      "   ---------------------------- ----------- 5/7 [huggingface-hub]\n",
      "   ---------------------------- ----------- 5/7 [huggingface-hub]\n",
      "   ---------------------------- ----------- 5/7 [huggingface-hub]\n",
      "   ---------------------------- ----------- 5/7 [huggingface-hub]\n",
      "   ---------------------------- ----------- 5/7 [huggingface-hub]\n",
      "   ---------------------------- ----------- 5/7 [huggingface-hub]\n",
      "   ---------------------------- ----------- 5/7 [huggingface-hub]\n",
      "   ---------------------------- ----------- 5/7 [huggingface-hub]\n",
      "   ---------------------------- ----------- 5/7 [huggingface-hub]\n",
      "   ---------------------------- ----------- 5/7 [huggingface-hub]\n",
      "   ---------------------------- ----------- 5/7 [huggingface-hub]\n",
      "   ---------------------------- ----------- 5/7 [huggingface-hub]\n",
      "   ---------------------------- ----------- 5/7 [huggingface-hub]\n",
      "   ---------------------------- ----------- 5/7 [huggingface-hub]\n",
      "   ---------------------------- ----------- 5/7 [huggingface-hub]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------------- 7/7 [datasets]\n",
      "\n",
      "Successfully installed datasets-4.0.0 dill-0.3.8 fsspec-2025.3.0 huggingface-hub-0.34.3 multiprocess-0.70.16 pyarrow-21.0.0 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "300bfaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German: Zwei junge weiÃŸe MÃ¤nner sind im Freien in der NÃ¤he vieler BÃ¼sche.\n",
      "English: Two young, White males are outside near many bushes.\n",
      "\n",
      "German: Mehrere MÃ¤nner mit Schutzhelmen bedienen ein Antriebsradsystem.\n",
      "English: Several men in hard hats are operating a giant pulley system.\n",
      "\n",
      "German: Ein kleines MÃ¤dchen klettert in ein Spielhaus aus Holz.\n",
      "English: A little girl climbing into a wooden playhouse.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "multi30k = load_dataset(\"bentrevett/multi30k\")\n",
    "train_data = multi30k[\"train\"]\n",
    "valid_data = multi30k[\"validation\"]\n",
    "test_data = multi30k[\"test\"]\n",
    "\n",
    "# Example: Print first 3 German-English pairs\n",
    "for i in range(3):\n",
    "    print(f\"German: {train_data[i]['de']}\")\n",
    "    print(f\"English: {train_data[i]['en']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81e0a103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hallo', ',', 'wie', 'geht', 'es', 'dir', '?']\n",
      "[1, 3, 8, 170, 61, 301, 7286, 2515, 2]\n",
      "[1, 3, 15, 889, 17, 1328, 2470, 2]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Load tokenizers\n",
    "spacy_ger = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenizer_de(text):\n",
    "    return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
    "\n",
    "def tokenizer_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# Test tokenizer\n",
    "text = \"Hallo, wie geht es dir?\"\n",
    "print(tokenizer_de(text))  # ['Hallo', ',', 'wie', 'geht', 'es', 'dir', '?']\n",
    "\n",
    "# Special tokens\n",
    "BOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "PAD_TOKEN = '<pad>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "\n",
    "# Load dataset from Hugging Face\n",
    "multi30k = load_dataset(\"bentrevett/multi30k\")\n",
    "train_data = multi30k[\"train\"]\n",
    "valid_data = multi30k[\"validation\"]\n",
    "test_data = multi30k[\"test\"]\n",
    "\n",
    "# Build vocabulary\n",
    "def yield_tokens(data, language='de'):\n",
    "    for example in data:\n",
    "        yield tokenizer_de(example['de']) if language == 'de' else tokenizer_en(example['en'])\n",
    "\n",
    "# German vocabulary\n",
    "german_vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_data, 'de'),\n",
    "    max_tokens=10000,\n",
    "    min_freq=2,\n",
    "    specials=[PAD_TOKEN, BOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
    ")\n",
    "german_vocab.set_default_index(german_vocab[UNK_TOKEN])\n",
    "\n",
    "# English vocabulary\n",
    "english_vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_data, 'en'),\n",
    "    max_tokens=10000,\n",
    "    min_freq=2,\n",
    "    specials=[PAD_TOKEN, BOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
    ")\n",
    "english_vocab.set_default_index(english_vocab[UNK_TOKEN])\n",
    "\n",
    "# Text transformation functions\n",
    "def german_transform(text):\n",
    "    return [german_vocab[BOS_TOKEN]] + german_vocab(tokenizer_de(text)) + [german_vocab[EOS_TOKEN]]\n",
    "\n",
    "def english_transform(text):\n",
    "    return [english_vocab[BOS_TOKEN]] + english_vocab(tokenizer_en(text)) + [english_vocab[EOS_TOKEN]]\n",
    "\n",
    "# Example usage\n",
    "print(german_transform(\"Hallo, wie geht es dir?\"))\n",
    "print(english_transform(\"Hello, how are you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5380012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout_value):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout_value)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_value)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        #embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290a38b7",
   "metadata": {},
   "source": [
    "### Hidden size of the Encoder and Decoder must be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37eb3ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout_value):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout_value)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_value)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        #x shape: (N) -> we want (1, N)\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        #embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        #outputs shape: (1, N, hidden_size)\n",
    "        predictions = self.fc(outputs)\n",
    "        #predictions shape: (1, N, output_size)\n",
    "        predictions = predictions.squeeze(0)\n",
    "        #predictions shape: (N, output_size)\n",
    "        return predictions, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a455510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(source.device)\n",
    "\n",
    "        hidden, cell = self.encoder(source)\n",
    "        x = target[0] #grab the first token from target\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            #output shape: (N, target_vocab_size)\n",
    "\n",
    "            outputs[t] = output\n",
    "\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            x = target[t] if random.random() < teacher_forcing_ratio else best_guess\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d42d290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "load_model = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size_encoder = len(german_vocab)\n",
    "input_size_decoder = len(english_vocab)\n",
    "output_size = len(english_vocab)\n",
    "encoder_embedding_size = 128\n",
    "decoder_embedding_size = 128\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "encoder_dropout_value = 0.5\n",
    "decoder_dropout_value = 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e60a6e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f'runs/Loss_plot')  \n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d9d4a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Convert to MapStyleDataset for shuffling/sorting\n",
    "train_dataset = to_map_style_dataset(train_data)\n",
    "valid_dataset = to_map_style_dataset(valid_data)\n",
    "test_dataset = to_map_style_dataset(test_data)\n",
    "\n",
    "# Create custom collate function with sorting\n",
    "def collate_batch(batch):\n",
    "    src_batch, trg_batch = [], []\n",
    "    for item in batch:\n",
    "        # Access German text with item['de'] and English with item['en']\n",
    "        src_batch.append(torch.tensor(german_transform(item['de'])))\n",
    "        trg_batch.append(torch.tensor(english_transform(item['en'])))\n",
    "    \n",
    "    # Sort by source length (descending)\n",
    "    sorted_indices = sorted(\n",
    "        range(len(src_batch)),\n",
    "        key=lambda i: len(src_batch[i]),\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    src_sorted = [src_batch[i] for i in sorted_indices]\n",
    "    trg_sorted = [trg_batch[i] for i in sorted_indices]\n",
    "    \n",
    "    # Pad sequences\n",
    "    src_padded = pad_sequence(src_sorted, padding_value=german_vocab[PAD_TOKEN])\n",
    "    trg_padded = pad_sequence(trg_sorted, padding_value=english_vocab[PAD_TOKEN])\n",
    "    \n",
    "    return src_padded, trg_padded\n",
    "\n",
    "# Create DataLoaders with sorting\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4f15ea69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, encoder_dropout_value).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e0d970dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size,output_size, num_layers, decoder_dropout_value).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5e454e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder_net, decoder_net).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9cf629ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=english_vocab[PAD_TOKEN])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "314a8664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, trg in loader:\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            output = model(src, trg, teacher_forcing_ratio=0)  # No teacher forcing\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1cc4ec1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 1/20 | Started: 16:47:42\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4103168 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m loss.backward()\n\u001b[32m     40\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m train_loss += loss.item()\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Progress tracking\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    276\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    277\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m                                \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    283\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     32\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m'\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     35\u001b[39m     torch.set_grad_enabled(prev_grad)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:141\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    130\u001b[39m     beta1, beta2 = group[\u001b[33m'\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    133\u001b[39m         group,\n\u001b[32m    134\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    138\u001b[39m         max_exp_avg_sqs,\n\u001b[32m    139\u001b[39m         state_steps)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:281\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    279\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\YN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:391\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    389\u001b[39m     denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m     denom = (\u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m).add_(eps)\n\u001b[32m    393\u001b[39m param.addcdiv_(exp_avg, denom, value=-step_size)\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4103168 bytes."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter('runs/translation_experiment')\n",
    "\n",
    "# Training variables\n",
    "best_valid_loss = float('inf')\n",
    "patience = 0\n",
    "max_patience = 3\n",
    "total_batches = len(train_loader)\n",
    "start_train_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Epoch header\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Started: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Training phase with progress bar\n",
    "    for batch_idx, (src, trg) in enumerate(train_loader):\n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        # Training steps\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        # Loss calculation\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Progress tracking\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - batch_start_time\n",
    "        batches_done = batch_idx + 1\n",
    "        batches_left = total_batches - batches_done\n",
    "        avg_batch_time = (current_time - epoch_start_time) / batches_done\n",
    "        remaining_time = avg_batch_time * batches_left\n",
    "        \n",
    "        # Print progress every 10% of batches (FIXED PARENTHESES)\n",
    "        if (batch_idx + 1) % max(1, len(train_loader) // 10) == 0:\n",
    "            print(f\"[Batch {batches_done}/{total_batches}] \"\n",
    "                  f\"Loss: {loss.item():.3f} | \"\n",
    "                  f\"Avg: {avg_batch_time:.2f}s/batch | \"\n",
    "                  f\"ETA: {str(timedelta(seconds=int(remaining_time)))}\")\n",
    "        \n",
    "        # TensorBoard logging\n",
    "        writer.add_scalar('Loss/train_batch', loss.item(), epoch * total_batches + batch_idx)\n",
    "    \n",
    "    # Validation phase\n",
    "    valid_start_time = time.time()\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    valid_loss /= len(valid_loader)\n",
    "    \n",
    "    # Epoch statistics\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # TensorBoard logging\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/valid', valid_loss, epoch)\n",
    "    writer.add_scalar('Time/epoch', epoch_time, epoch)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Epoch {epoch+1} Summary:\")\n",
    "    print(f\"  Training Loss: {train_loss:.3f} | PPL: {math.exp(train_loss):7.3f}\")\n",
    "    print(f\"  Validation Loss: {valid_loss:.3f} | PPL: {math.exp(valid_loss):7.3f}\")\n",
    "    print(f\"  Time: {str(timedelta(seconds=int(epoch_time)))}\")\n",
    "    print(f\"  Total Elapsed: {str(timedelta(seconds=int(time.time() - start_train_time)))}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        patience = 0\n",
    "        print(\"â†— New best model saved!\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= max_patience:\n",
    "            print(f\"Early stopping after {patience} epochs without improvement\")\n",
    "            break\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02095f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
